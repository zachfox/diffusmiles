{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import functools\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy import special as sf\n",
    "from scipy.stats import binom as spbinom\n",
    "from numba import njit,float64,int64,jit\n",
    "from numba.types import UniTuple\n",
    "from matplotlib import pyplot as plt\n",
    "import numba_scipy\n",
    "import gc\n",
    "import os\n",
    "from utils import save_checkpoint_withEval as save_checkpoint\n",
    "from utils import restore_checkpoint_withEval as restore_checkpoint\n",
    "from loadDataPipeline import generateData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = nn.functional.relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.cpp_extension import load\n",
    "from models import ncsnpp\n",
    "from configs.vp import cifar10_ncsnpp_continuous as configLoader\n",
    "from models import utils as mutils\n",
    "from models.ema import ExponentialMovingAverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the ML model from Song et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config =  configLoader.get_config()\n",
    "config.training.batch_size=128\n",
    "config.training.snapshot_freq_for_preemption=1000\n",
    "config.training.snapshot_freq=50000\n",
    "config.training.log_freq=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.data.dataset='MNIST'\n",
    "config.data.image_size=32\n",
    "config.data.num_channels=3\n",
    "config.data.random_flip=False\n",
    "config.model.nf=64\n",
    "config.model.name='ncsnpp'\n",
    "config.model.num_scales=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, eval_ds, scaler = generateData(config,'mnist')\n",
    "\n",
    "train_iter = iter(train_ds)\n",
    "eval_iter = iter(eval_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify observation times (noise schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tEnd = 15. #(approximately)\n",
    "T = 1000\n",
    "T = T+1\n",
    "observationTimes1 = np.linspace(0, 1, 201)[1:]\n",
    "observationTimes2 = np.linspace(1, tEnd, 801)[1:]\n",
    "observationTimes = np.hstack((observationTimes1,observationTimes2))\n",
    "T = T-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(observationTimes)\n",
    "plt.xlabel('computational time step')\n",
    "plt.ylabel('physical time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytically derived reverse-time transition rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brTable = np.zeros((256,256,T))\n",
    "for tIndex in range(T):\n",
    "    p = np.exp(-observationTimes[tIndex])\n",
    "    for n in range(256):\n",
    "        for m in range(n):\n",
    "            brTable[n,m,tIndex] = n-m \n",
    "        brTable[n,n,tIndex] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDataLinearDegradation(img, tIndex, t):\n",
    "    \n",
    "    nx, ny, nz = img.shape\n",
    "    \n",
    "    p = np.exp(-t)\n",
    "    \n",
    "    output_image = np.random.binomial(img.astype('int32'), p)\n",
    "    birthRate = np.zeros_like(output_image).astype('float32')\n",
    "    \n",
    "    for sx in range(nx):\n",
    "        for sy in range(ny):\n",
    "            for sz in range(nz):\n",
    "\n",
    "                n0 = np.int64(img[sx, sy, sz])\n",
    "                nt = np.int64(output_image[sx, sy, sz])\n",
    "                \n",
    "                birthRate[sx,sy,sz] = brTable[n0, nt, tIndex]\n",
    "    \n",
    "    width = 255.0/2*p\n",
    "    mean_v = 255.0/2*p\n",
    "    \n",
    "    return (output_image-mean_v)/width, birthRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateBatchData(imgBatch, T, observationTimes):\n",
    "    \n",
    "    imgBatchNumpy = imgBatch.detach().cpu().numpy()\n",
    "    output_image_batch = np.zeros_like(imgBatchNumpy)\n",
    "    birthRate_batch = np.zeros_like(imgBatchNumpy)\n",
    "    tIndexArray = np.random.choice(T, size=len(output_image_batch))\n",
    "    \n",
    "    for i in range(len(output_image_batch)):\n",
    "        \n",
    "        testImage = np.round(np.transpose((1+imgBatchNumpy[i,:,:,:])/2*255, [1,2,0]))\n",
    "        output_image, birth_rate = generateDataLinearDegradation(testImage, tIndexArray[i], observationTimes[tIndexArray[i]])\n",
    "        \n",
    "        output_image_batch[i,:] = np.transpose(output_image[:], [2,0,1])\n",
    "        birthRate_batch[i,:] = np.transpose(birth_rate[:], [2,0,1])\n",
    "\n",
    "    return np.repeat(output_image_batch,3,axis=1), birthRate_batch, tIndexArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_batch = torch.from_numpy(next(train_iter)['image']._numpy()).to(config.device).float()\n",
    "train_batch = train_batch.permute(0, 3, 1, 2)\n",
    "train_batch = scaler(train_batch)\n",
    "\n",
    "output_image_batch, birthRate_batch, tIndexArray = generateBatchData(train_batch, T, observationTimes)\n",
    "birthRate_batch_torch = torch.tensor(birthRate_batch).to(config.device)\n",
    "tIndexArray = torch.tensor(tIndexArray).to(config.device)\n",
    "\n",
    "for i in range(20):\n",
    "\n",
    "    #testImage = np.round(np.transpose((1+train_batch[i].detach().cpu().numpy())/2*255, [1,2,0]))\n",
    "    #targetTime = np.around(np.random.uniform(low=0.00001, high=15), 4)\n",
    "    #output_image, birthRate = generateDataLinearDegradation(testImage, targetTime)\n",
    "    \n",
    "    testImage = np.round(np.transpose((1+train_batch[i].detach().cpu().numpy())/2*255, [1,2,0]))\n",
    "    output_image = np.transpose((1+output_image_batch[i,:,:,:])/2*255, [1,2,0])\n",
    "    birthRate = np.transpose(birthRate_batch[i,:,:,:], [1,2,0])\n",
    "    targetTime = np.around(observationTimes[tIndexArray[i]], 4)\n",
    "    \n",
    "    fig, ax = plt.subplots(1,3, figsize=(4.8,1.5))\n",
    "    \n",
    "    ax[0].imshow(testImage/255.)\n",
    "    \n",
    "    if np.amax(output_image)!=0:\n",
    "        ax[1].imshow(output_image/np.amax(output_image))\n",
    "    else:\n",
    "        ax[1].imshow(output_image)\n",
    "        \n",
    "    ax[1].set_title('$t='+str(targetTime)+'$')\n",
    "\n",
    "    if np.amax(birthRate)-np.amin(birthRate)!=0:\n",
    "        ax[2].imshow((birthRate-np.amin(birthRate))/(np.amax(birthRate)-np.amin(birthRate)))\n",
    "    else:\n",
    "        ax[2].imshow(birthRate)\n",
    "    \n",
    "    for j in range(3):\n",
    "        \n",
    "        ax[j].set_xticklabels('')\n",
    "        ax[j].set_yticklabels('')\n",
    "    \n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate an ML model to learn the transition rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model = mutils.create_model(config)\n",
    "score_fn = mutils.get_model_fn(score_model, train=True)\n",
    "optimizer = torch.optim.Adam(score_model.parameters(),lr=config.optim.lr) \n",
    "\n",
    "ema = ExponentialMovingAverage(score_model.parameters(), decay=config.model.ema_rate)\n",
    "\n",
    "train_batch = torch.from_numpy(next(train_iter)['image']._numpy()).to(config.device).float()\n",
    "train_batch = train_batch.permute(0, 3, 1, 2)\n",
    "imgBatch = scaler(train_batch)\n",
    "\n",
    "workdir = 'linearDegradation-mnist'\n",
    "\n",
    "state = dict(optimizer=optimizer, model=score_model, ema=ema, lossHistory=[], evalLossHistory=[], step=0)\n",
    "\n",
    "checkpoint_dir = os.path.join(workdir, \"checkpoints\")\n",
    "checkpoint_meta_dir = os.path.join(workdir, \"checkpoints-meta\", \"checkpoint.pth\")\n",
    "tf.io.gfile.makedirs(checkpoint_dir)\n",
    "tf.io.gfile.makedirs(os.path.dirname(checkpoint_meta_dir))\n",
    "state = restore_checkpoint(checkpoint_meta_dir, state, config.device)\n",
    "initial_step = int(state['step'])\n",
    "lossHistory = state['lossHistory']\n",
    "evalLossHistory = state['evalLossHistory']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for step in range(initial_step, config.training.n_iters):\n",
    "    \n",
    "    train_batch = torch.from_numpy(next(train_iter)['image']._numpy()).to(config.device).float()\n",
    "    train_batch = train_batch.permute(0, 3, 1, 2)\n",
    "    train_batch = scaler(train_batch)\n",
    "\n",
    "    output_image_batch, birthRate_batch, tIndexArray = generateBatchData(train_batch, T, observationTimes)\n",
    "    birthRate_batch_torch = torch.from_numpy(birthRate_batch).to(config.device)\n",
    "    output_image_batch_torch = torch.from_numpy(output_image_batch).to(config.device)\n",
    "    tIndexArray = torch.from_numpy(tIndexArray).to(config.device)\n",
    "\n",
    "    y = relu(score_fn(output_image_batch_torch, tIndexArray))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = torch.mean(torch.square(y-birthRate_batch_torch))\n",
    "   \n",
    "    loss.backward()\n",
    "    state['ema'].update(state['model'].parameters())\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    lossHistory.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    if step != 0 and step % config.training.snapshot_freq_for_preemption == 0:\n",
    "        save_checkpoint(checkpoint_meta_dir, state)\n",
    "        \n",
    "    if step != 0 and step % config.training.snapshot_freq == 0 or step == config.training.n_iters:\n",
    "        save_step = step // config.training.snapshot_freq\n",
    "        save_checkpoint(os.path.join(checkpoint_dir, f'checkpoint_{save_step}.pth'), state)    \n",
    "    \n",
    "    if np.mod(step, config.training.log_freq)==0:\n",
    "        \n",
    "        eval_batch = torch.from_numpy(next(eval_iter)['image']._numpy()).to(config.device).float()\n",
    "        eval_batch = eval_batch.permute(0, 3, 1, 2)\n",
    "        eval_batch = scaler(eval_batch)\n",
    "        \n",
    "        output_image_batch, birthRate_batch, tIndexArray = generateBatchData(eval_batch, T, observationTimes)\n",
    "        birthRate_batch_torch = torch.from_numpy(birthRate_batch).to(config.device)\n",
    "        output_image_batch_torch = torch.from_numpy(output_image_batch).to(config.device)\n",
    "        tIndexArray = torch.from_numpy(tIndexArray).to(config.device)\n",
    "\n",
    "        ema.store(score_model.parameters())\n",
    "        ema.copy_to(score_model.parameters())\n",
    "        \n",
    "        y = relu(score_fn(output_image_batch_torch, tIndexArray))\n",
    "        loss = torch.mean(torch.square(y-birthRate_batch_torch))\n",
    "        \n",
    "        ema.restore(score_model.parameters())\n",
    "        \n",
    "        evalLossHistory.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        print(f'current iter: {step}, loss: {lossHistory[-1]}, eval loss: {evalLossHistory[-1]}')\n",
    "        \n",
    "    state['step'] = step\n",
    "    state['lossHistory'] = lossHistory\n",
    "    state['evalLossHistory'] = evalLossHistory\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchSDE",
   "language": "python",
   "name": "torchsde"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
